{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, KFold, GridSearchCV, StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from collections import Counter\n",
        "import warnings"
      ],
      "metadata": {
        "id": "8Ya6z3n-C4V9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Suppress specific warnings from scikit-learn for cleaner output\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn\")\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn\")\n"
      ],
      "metadata": {
        "id": "LhNKBIM4C5eM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df = pd.read_csv('/content/drive/MyDrive/softwarica/machine-learning/air-quality-prediction-classification/compiled/kathmandu_pm25_class_2020_1_to_2025_4_dataset.csv')"
      ],
      "metadata": {
        "id": "77dx3nL-C69E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Label Encode the target variable 'pm25_class'\n",
        "le = LabelEncoder()\n",
        "final_df['pm25_class'] = le.fit_transform(final_df['pm25_class'])"
      ],
      "metadata": {
        "id": "W9Kz7ZR8DMml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureEngineer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    A custom transformer to perform complex feature engineering steps:\n",
        "    - Clipping numerical features to specified bounds.\n",
        "    - Transforming wind direction into sine/cosine components and binary flags.\n",
        "    - Creating an 'is_windy' flag and handling rare 'condition' categories.\n",
        "    - Generating lagged features for key variables including the target.\n",
        "    - Dropping the 'date' column and handling NaNs.\n",
        "    \"\"\"\n",
        "    def __init__(self, clipping_bounds=None, rare_condition_threshold=100, lag_features=None, lags=None):\n",
        "        # Define default clipping bounds\n",
        "        self.clipping_bounds = clipping_bounds if clipping_bounds is not None else {\n",
        "            'temperature': {'lower': 1.5, 'upper': 35.4},\n",
        "            'pressure': {'lower': 855, 'upper': 885},\n",
        "            'dew_point': {'lower': -10, 'upper': 25},\n",
        "            'humidity': {'lower': 0, 'upper': 100}\n",
        "        }\n",
        "        self.rare_condition_threshold = rare_condition_threshold\n",
        "        # Define default features for lagging\n",
        "        self.lag_features = lag_features if lag_features is not None else ['temperature', 'humidity', 'wind_speed', 'dew_point', 'pressure']\n",
        "        self.lags = lags if lags is not None else [1, 2, 3]\n",
        "        # Mapping for wind directions to degrees\n",
        "        self.direction_map = {\n",
        "            'N': 0, 'NNE': 22.5, 'NE': 45, 'ENE': 67.5,\n",
        "            'E': 90, 'ESE': 112.5, 'SE': 135, 'SSE': 157.5,\n",
        "            'S': 180, 'SSW': 202.5, 'SW': 225, 'WSW': 247.5,\n",
        "            'W': 270, 'WNW': 292.5, 'NW': 315, 'NNW': 337.5,\n",
        "            'CALM': np.nan, 'VAR': np.nan\n",
        "        }\n",
        "        self.rare_conditions_ = None # To store rare conditions learned during fit\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Ensure X is a DataFrame for proper column operations\n",
        "        if not isinstance(X, pd.DataFrame):\n",
        "            X = pd.DataFrame(X)\n",
        "\n",
        "        # Learn rare conditions from the 'condition' column\n",
        "        if 'condition' in X.columns:\n",
        "            temp_condition_base = X['condition'].str.replace(' / Windy', '', regex=False)\n",
        "            condition_counts = temp_condition_base.value_counts()\n",
        "            self.rare_conditions_ = condition_counts[condition_counts < self.rare_condition_threshold].index\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_transformed = X.copy()\n",
        "\n",
        "        # 1. Clipping numerical features according to predefined bounds\n",
        "        for col, bounds in self.clipping_bounds.items():\n",
        "            if col in X_transformed.columns:\n",
        "                X_transformed[col] = X_transformed[col].clip(lower=bounds['lower'], upper=bounds['upper'])\n",
        "\n",
        "        # 2. Wind Transformation: Convert wind direction to numerical features\n",
        "        if 'wind' in X_transformed.columns:\n",
        "            X_transformed['wind_deg'] = X_transformed['wind'].map(self.direction_map)\n",
        "            X_transformed['is_calm'] = (X_transformed['wind'] == 'CALM').astype(int)\n",
        "            X_transformed['is_var'] = (X_transformed['wind'] == 'VAR').astype(int)\n",
        "            # Fill NaN degrees (from 'CALM'/'VAR') with 0 for trigonometric encoding\n",
        "            X_transformed['wind_deg'] = X_transformed['wind_deg'].fillna(0)\n",
        "            X_transformed['wind_sin'] = np.sin(np.deg2rad(X_transformed['wind_deg']))\n",
        "            X_transformed['wind_cos'] = np.cos(np.deg2rad(X_transformed['wind_deg']))\n",
        "            # Drop the original 'wind' column and the intermediate 'wind_deg'\n",
        "            X_transformed.drop(columns=['wind', 'wind_deg'], inplace=True)\n",
        "\n",
        "        # 3. Condition Transformation: Create 'is_windy' and handle rare categories\n",
        "        if 'condition' in X_transformed.columns:\n",
        "            X_transformed['is_windy'] = X_transformed['condition'].str.contains('/ Windy', na=False).astype(int)\n",
        "            X_transformed['condition_base'] = X_transformed['condition'].str.replace(' / Windy', '', regex=False)\n",
        "            # Replace rare conditions with 'Other' based on what was learned during fit\n",
        "            if self.rare_conditions_ is not None:\n",
        "                X_transformed['condition'] = X_transformed['condition_base'].replace(self.rare_conditions_, 'Other')\n",
        "            else:\n",
        "                # Fallback if fit was not called or no rare conditions were identified\n",
        "                X_transformed['condition'] = X_transformed['condition_base']\n",
        "            X_transformed.drop(columns=['condition_base'], inplace=True)\n",
        "\n",
        "        # 4. Add lagged features: Requires 'date' column for sorting and 'pm25_class' for lagging.\n",
        "        # This transformer is designed to be applied to the full dataset (including target)\n",
        "        # before splitting into X and y for train/test.\n",
        "        if 'date' in X_transformed.columns:\n",
        "            X_transformed.sort_values('date', inplace=True) # Sort by date for correct lagging\n",
        "            # Lag numerical features\n",
        "            for feature in self.lag_features:\n",
        "                if feature in X_transformed.columns:\n",
        "                    for lag in self.lags:\n",
        "                        X_transformed[f'{feature}_lag{lag}'] = X_transformed[feature].shift(lag)\n",
        "            # Lag the PM2.5 class (target) as a feature\n",
        "            if 'pm25_class' in X_transformed.columns:\n",
        "                for lag in self.lags:\n",
        "                    X_transformed[f'pm25_class_lag{lag}'] = X_transformed['pm25_class'].shift(lag)\n",
        "\n",
        "            X_transformed.drop(columns=['date'], inplace=True) # Drop date column after lagging\n",
        "\n",
        "        # Drop rows with NaNs created by lagging (first few rows will have NaNs)\n",
        "        X_transformed.dropna(inplace=True)\n",
        "\n",
        "        # Reset index to ensure clean DataFrame for subsequent steps in the pipeline\n",
        "        return X_transformed.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "VZomNqx-DMhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KRjP2wBMbcnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Apply the Custom Feature Engineer and split X, y ---\n",
        "# The FeatureEngineer needs to be fitted and transformed on the full dataset (final_df)\n",
        "# before splitting into X and y, because it creates lagged features from the target.\n",
        "df_engineered = FeatureEngineer().fit_transform(final_df.copy())\n"
      ],
      "metadata": {
        "id": "VRZxpZMGDMed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate features (X) and target (y) from the engineered DataFrame\n",
        "X = df_engineered.drop(columns=['pm25_class'])\n",
        "y = df_engineered['pm25_class']"
      ],
      "metadata": {
        "id": "bSGbRg5hDcK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Train Test Split ---\n",
        "# Split the dataset into training+validation and test sets\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
        "# Split the training+validation set into actual training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.1765, random_state=42, stratify=y_train_val)\n"
      ],
      "metadata": {
        "id": "sUXh3ieGDcIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
        "categorical_cols = X.select_dtypes(include='object').columns.tolist()"
      ],
      "metadata": {
        "id": "BuhzCDq8DcFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if 'condition' in numerical_cols:\n",
        "    numerical_cols.remove('condition')\n",
        "if 'condition' not in categorical_cols:\n",
        "    categorical_cols.append('condition')"
      ],
      "metadata": {
        "id": "kWkCvsw7Douf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_cols),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_cols)\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")"
      ],
      "metadata": {
        "id": "B_YKxDK5DosH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Each pipeline consists of the preprocessor, SMOTE for oversampling, and a classifier.\n",
        "pipeline_lr = Pipeline([\n",
        "    ('preprocessor', preprocessor), # Applies scaling and one-hot encoding\n",
        "    ('smote', SMOTE(random_state=42)), # Handles class imbalance on the training data\n",
        "    ('classifier', LogisticRegression(random_state=42))  #max_iter=2000, solver='liblinear'))\n",
        "])\n",
        "pipeline_rf = Pipeline([\n",
        "    ('preprocessor', preprocessor), # Applies scaling and one-hot encoding\n",
        "    ('smote', SMOTE(random_state=42)), # Handles class imbalance on the training data\n",
        "    ('classifier', RandomForestClassifier(random_state=42, n_jobs=-1))\n",
        "])\n",
        "\n",
        "pipeline_xgb = Pipeline([\n",
        "    ('preprocessor', preprocessor), # Applies scaling and one-hot encoding\n",
        "    ('smote', SMOTE(random_state=42)), # Handles class imbalance on the training data\n",
        "    ('classifier', xgb.XGBClassifier(random_state=42, n_jobs=-1, eval_metric='mlogloss', verbosity=0))\n",
        "])"
      ],
      "metadata": {
        "id": "kAi_S1aeDope"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# T specify the hyperparameters to tune for each classifier within the pipeline.\n",
        "param_grids = {\n",
        "     \"Logistic Regression\": {\n",
        "        'classifier__C': [0.1, 1, 10],\n",
        "    },\n",
        "    \"Random Forest\": {\n",
        "        'classifier__n_estimators': [100, 200], # Number of trees\n",
        "        'classifier__max_depth': [10, 20],      # Maximum depth of each tree\n",
        "    },\n",
        "    \"XGBoost\": {\n",
        "        'classifier__n_estimators': [100, 200], # Number of boosting rounds\n",
        "        'classifier__max_depth': [3, 6],        # Maximum depth of each tree\n",
        "        'classifier__learning_rate': [0.1, 0.2],# Step size shrinkage to prevent overfitting\n",
        "    },\n",
        "}"
      ],
      "metadata": {
        "id": "Zpe_a_KUDomh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Train and Evaluate Pipelines using GridSearchCV\n",
        "# Determine cross-validation folds based on the minimum class count in the training data\n",
        "min_class_count = y_train.value_counts().min()\n",
        "cv_folds = 2 if min_class_count < 5 else 5 # Use 2 folds if min class count is very low, else 5\n",
        "cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
        "\n",
        "best_models = {}\n",
        "\n",
        "# GridSearchCV for pipelines\n",
        "print(\"Training Logistic Regression Pipeline...\")\n",
        "grid_lr = GridSearchCV(pipeline_lr, param_grids[\"Logistic Regression\"], cv=cv, scoring='f1_macro', n_jobs=-1)\n",
        "grid_lr.fit(X_train, y_train) # Fit on X_train; SMOTE and preprocessing are handled within the pipeline\n",
        "\n",
        "print(\"Training Random Forest Pipeline...\")\n",
        "grid_rf = GridSearchCV(pipeline_rf, param_grids[\"Random Forest\"], cv=cv, scoring='f1_macro', n_jobs=-1)\n",
        "grid_rf.fit(X_train, y_train) # Fit on X_train; SMOTE and preprocessing are handled within the pipeline\n",
        "\n",
        "print(\"Training XGBoost Pipeline...\")\n",
        "grid_xgb = GridSearchCV(pipeline_xgb, param_grids[\"XGBoost\"], cv=cv, scoring='f1_macro', n_jobs=-1)\n",
        "grid_xgb.fit(X_train, y_train) # Fit on X_train; SMOTE and preprocessing are handled within the pipeline\n",
        "\n",
        "# Store the best models from each pipeline\n",
        "best_models[\"Logistic Regression\"] = grid_lr.best_estimator_\n",
        "best_models[\"Random Forest\"] = grid_rf.best_estimator_\n",
        "best_models[\"XGBoost\"] = grid_xgb.best_estimator_\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUnqUVE4crZn",
        "outputId": "9533a73d-6ef6-4cea-fc09-1f7cfe3ed65c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Logistic Regression Pipeline...\n",
            "Training Random Forest Pipeline...\n",
            "Training XGBoost Pipeline...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gaa_xbSQxftM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nBest Parameters for Logistic Regression:\", grid_lr.best_params_)\n",
        "print(\"Logistic Regression CV F1 Score:\", grid_lr.best_score_)\n",
        "\n",
        "print(\"\\nLogistic Regression Validation Report:\")\n",
        "val_preds_lr = grid_lr.predict(X_val)\n",
        "print(classification_report(y_val, val_preds_lr, zero_division=0))\n",
        "\n",
        "val_accuracy_lr = accuracy_score(y_val, val_preds_lr)\n",
        "print(f\"Logistic Regression Validation Accuracy: {val_accuracy_lr:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G39j3nJ1sZZa",
        "outputId": "2438dcfc-daa6-4902-b11c-10affe31ca30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Best Parameters for Logistic Regression: {'classifier__C': 0.1}\n",
            "Logistic Regression CV F1 Score: 0.5613921462375991\n",
            "\n",
            "Logistic Regression Validation Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.21      0.57      0.31         7\n",
            "           1       0.54      0.56      0.55        34\n",
            "           2       0.40      0.54      0.46        41\n",
            "           3       0.87      0.65      0.74       140\n",
            "           4       0.66      0.76      0.71        51\n",
            "\n",
            "    accuracy                           0.64       273\n",
            "   macro avg       0.54      0.62      0.55       273\n",
            "weighted avg       0.70      0.64      0.66       273\n",
            "\n",
            "Logistic Regression Validation Accuracy: 0.6410\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nBest Parameters for Random Forest:\", grid_rf.best_params_)\n",
        "print(\"Random Forest CV F1 Score:\", grid_rf.best_score_)\n",
        "\n",
        "print(\"\\nRandom Forest Validation Report:\")\n",
        "val_preds_rf = grid_rf.predict(X_val)\n",
        "print(classification_report(y_val, val_preds_rf, zero_division=0))\n",
        "\n",
        "val_accuracy_rf = accuracy_score(y_val, val_preds_rf)\n",
        "print(f\"Random Forest Validation Accuracy: {val_accuracy_rf:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yg0dliiYd-Kh",
        "outputId": "9bace05a-10b6-4d8f-d688-5a50b7ea81fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Best Parameters for Random Forest: {'classifier__max_depth': 10, 'classifier__n_estimators': 200}\n",
            "Random Forest CV F1 Score: 0.6519071819790677\n",
            "\n",
            "Random Forest Validation Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.71      0.67         7\n",
            "           1       0.73      0.65      0.69        34\n",
            "           2       0.55      0.68      0.61        41\n",
            "           3       0.84      0.77      0.81       140\n",
            "           4       0.64      0.71      0.67        51\n",
            "\n",
            "    accuracy                           0.73       273\n",
            "   macro avg       0.68      0.70      0.69       273\n",
            "weighted avg       0.74      0.73      0.73       273\n",
            "\n",
            "Random Forest Validation Accuracy: 0.7289\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\nBest Parameters for XGBoost:\", grid_xgb.best_params_)\n",
        "print(\"XGBoost CV F1 Score:\", grid_xgb.best_score_)\n",
        "\n",
        "# Evaluate XGBoost\n",
        "print(\"\\nXGBoost Validation Report:\")\n",
        "val_preds_xgb = grid_xgb.predict(X_val)\n",
        "print(classification_report(y_val, val_preds_xgb, zero_division=0))\n",
        "\n",
        "val_accuracy_xgb = accuracy_score(y_val, val_preds_xgb)\n",
        "print(f\"XGBoost Validation Accuracy: {val_accuracy_xgb:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzNx3mzzcrWz",
        "outputId": "3bbd2b8e-de6e-4398-8e29-9d7fd4f6b754"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Best Parameters for XGBoost: {'classifier__learning_rate': 0.1, 'classifier__max_depth': 3, 'classifier__n_estimators': 200}\n",
            "XGBoost CV F1 Score: 0.6573208968693166\n",
            "\n",
            "XGBoost Validation Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.43      0.43      0.43         7\n",
            "           1       0.64      0.74      0.68        34\n",
            "           2       0.57      0.56      0.57        41\n",
            "           3       0.84      0.79      0.81       140\n",
            "           4       0.66      0.73      0.69        51\n",
            "\n",
            "    accuracy                           0.73       273\n",
            "   macro avg       0.63      0.65      0.64       273\n",
            "weighted avg       0.73      0.73      0.73       273\n",
            "\n",
            "XGBoost Validation Accuracy: 0.7253\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Final Test Evaluation ---\n",
        "# Evaluate the best trained models on the unseen test set\n",
        "print(\"\\nFinal Test Evaluation:\")\n",
        "for name, model in best_models.items():\n",
        "    test_preds = model.predict(X_test)\n",
        "    print(f\"\\n{name} Test Report:\")\n",
        "    print(classification_report(y_test, test_preds, zero_division=0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwe0oED5crT8",
        "outputId": "1b15922f-ee3e-4757-bc72-41476f86a426"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Test Evaluation:\n",
            "\n",
            "Random Forest Test Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.57      0.62         7\n",
            "           1       0.63      0.65      0.64        34\n",
            "           2       0.69      0.61      0.65        41\n",
            "           3       0.86      0.85      0.85       139\n",
            "           4       0.69      0.77      0.73        52\n",
            "\n",
            "    accuracy                           0.77       273\n",
            "   macro avg       0.71      0.69      0.70       273\n",
            "weighted avg       0.77      0.77      0.77       273\n",
            "\n",
            "\n",
            "XGBoost Test Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.71      0.77         7\n",
            "           1       0.62      0.59      0.61        34\n",
            "           2       0.64      0.56      0.60        41\n",
            "           3       0.85      0.89      0.87       139\n",
            "           4       0.74      0.75      0.74        52\n",
            "\n",
            "    accuracy                           0.77       273\n",
            "   macro avg       0.74      0.70      0.72       273\n",
            "weighted avg       0.77      0.77      0.77       273\n",
            "\n",
            "\n",
            "Logistic Regression Test Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.27      0.43      0.33         7\n",
            "           1       0.53      0.53      0.53        34\n",
            "           2       0.44      0.59      0.51        41\n",
            "           3       0.87      0.73      0.79       139\n",
            "           4       0.67      0.75      0.71        52\n",
            "\n",
            "    accuracy                           0.68       273\n",
            "   macro avg       0.56      0.60      0.57       273\n",
            "weighted avg       0.71      0.68      0.69       273\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Assuming 'best_models' dictionary contains your trained pipelines\n",
        "# best_models[\"Random Forest\"] and best_models[\"XGBoost\"]\n",
        "joblib.dump(best_models[\"Random Forest\"], 'random_forest_pipeline.pkl')\n",
        "joblib.dump(best_models[\"XGBoost\"], 'xgboost_pipeline.pkl')\n",
        "\n",
        "# Also save the LabelEncoder and the X_train_columns for consistent preprocessing\n",
        "joblib.dump(le, 'label_encoder.pkl')\n",
        "joblib.dump(X_train.columns.tolist(), 'X_train_columns.pkl')\n",
        "\n",
        "# Save the accuracy scores\n",
        "joblib.dump(val_accuracy_rf, 'rf_validation_accuracy.pkl')\n",
        "joblib.dump(val_accuracy_xgb, 'xgb_validation_accuracy.pkl')\n",
        "\n",
        "print(\"Models, LabelEncoder, X_train_columns, and Validation Accuracies saved successfully!\")"
      ],
      "metadata": {
        "id": "SK5q8R-WcrRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xMrBXu0bILpQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}